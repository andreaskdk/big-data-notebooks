{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Movie Reviews\n",
    "\n",
    "In this hands-on exercise we will look at data, which is not bounded. In many applications data is continuously updated. The data that we will be working with comes from the Internet Movie Database (IMDB) app. Users who have set their app up to connect with Twitter will automatically produce a tweet everytime they rate a movie in the app. It is possible to subscribe to tweets as they are produced, but for simplicity we will simulate this process by streaming historic data.\n",
    "\n",
    "To work with streaming data in Spark we need to create a StreamingContext that plays a similar role to the SparkContext of a batch application. We also need to set an interval of how often we want to process data. Here we will set it to 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "batch_interval=10\n",
    "stream_context = StreamingContext(sc, batch_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now manipulate the streaming data similarly to what we would do with batch data, but the difference is that the processing is repeated every 10 seconds with the data that have arrived since last run.\n",
    "\n",
    "We are faking the stream of reviews by hooking up to a bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_of_reviews=stream_context.textFileStream(\"gs://big-data-streaming-examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we process data we would normally update a database with results as they come along. For simplicity we will just keep a local dictionary with that can store data.\n",
    "\n",
    "In the dictionary **local_data** we store three variables:\n",
    "- The total number of reviews we have processed (**total_count**)\n",
    "- One example of a recent review (**one_line**)\n",
    "- The time of the latest data batch (**latest_processing_time**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data={}\n",
    "local_data[\"total_count\"]=0\n",
    "local_data[\"one_line\"]=\"\"\n",
    "local_data[\"latest_processing_time\"]=\"\"\n",
    "\n",
    "\n",
    "def count_and_keep_one(time, rdd):\n",
    "    data=rdd.collect()\n",
    "    local_data[\"latest_processing_time\"]=time\n",
    "    local_data[\"total_count\"] += len(data)\n",
    "    if len(data)>0:\n",
    "        local_data[\"one_line\"]=data[0]\n",
    "    \n",
    "\n",
    "stream_of_reviews.foreachRDD(count_and_keep_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out what we have in the *database*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of lines processed: \"+str(local_data[\"total_count\"]))\n",
    "print(\"Latest processing time: \"+str(local_data[\"latest_processing_time\"]))\n",
    "print(\"Example of a line from latest batch: \"+local_data[\"one_line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any processing can happen we need to start the streaming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can stop the process again when we are done. The boolean argument indicates whether the SparkContext should be destroyed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to store and update the distribution of ratings as they arrive. This is challenging for a number of reasons. One important reason is that errors are not outputted to the notebook. The errors can, however, be found via the SparkUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "           var el=document.createElement(\"h3\");\n",
       "           var ela=document.createElement(\"a\");\n",
       "           ela.innerHTML=\"SparkUI\";\n",
       "           ela.href=window.location.protocol + '//' + window.location.hostname + ':8088/proxy/application_1554654931793_0002/';\n",
       "           ela.target=\"_blank\";\n",
       "           el.append(ela);\n",
       "           \n",
       "           element.append(el);\n",
       "           "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "Javascript(\"\"\"\n",
    "           var el=document.createElement(\"h3\");\n",
    "           var ela=document.createElement(\"a\");\n",
    "           ela.innerHTML=\"SparkUI\";\n",
    "           ela.href=window.location.protocol + '//' + window.location.hostname + ':8088/proxy/\"\"\" \\\n",
    "           + spark.sparkContext.applicationId \\\n",
    "           + \"\"\"/';\n",
    "           ela.target=\"_blank\";\n",
    "           el.append(ela);\n",
    "           \n",
    "           element.append(el);\n",
    "           \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}