{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Movie Reviews\n",
    "\n",
    "In this hands-on exercise we will look at data, which is not bounded. In many applications data is continuously updated. The data that we will be working with comes from the Internet Movie Database (IMDB) app. Users who have set their app up to connect with Twitter will automatically produce a tweet everytime they rate a movie in the app. It is possible to subscribe to tweets as they are produced, but for simplicity we will simulate this process by streaming historic data.\n",
    "\n",
    "To work with streaming data in Spark we need to create a StreamingContext that plays a similar role to the SparkContext of a batch application. We also need to set an interval of how often we want to process data. Here we will set it to 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "batch_interval=10\n",
    "stream_context = StreamingContext(sc, batch_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now manipulate the streaming data similarly to what we would do with batch data, but the difference is that the processing is repeated every 10 seconds with the data that have arrived since last run.\n",
    "\n",
    "We are faking the stream of reviews by hooking up to a bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_of_reviews=stream_context.textFileStream(\"gs://big-data-streaming-examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_data={}\n",
    "local_data[\"total_count\"]=0\n",
    "local_data[\"one_line\"]=\"\"\n",
    "local_data[\"latest_processing_time\"]=\"\"\n",
    "\n",
    "\n",
    "\n",
    "def count_and_keep_one(time, rdd):\n",
    "    data=rdd.collect()\n",
    "    local_data[\"latest_processing_time\"]=time\n",
    "    local_data[\"total_count\"] += len(data)\n",
    "    if len(data)>0:\n",
    "        local_data[\"one_line\"]=data[0]\n",
    "    \n",
    "\n",
    "stream_of_reviews.map(lambda x: x).foreachRDD(count_and_keep_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines processed: 2520\n",
      "Latest processing time: 2019-04-06 16:35:10\n",
      "Example of a line from latest batch: {\"movie\": \"Killer Elite (2011)\", \"time\": \"2013-05-16 19:47:04\", \"user\": \"134511613\", \"timestamp\": 1368733624, \"rating\": 6}\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of lines processed: \"+str(local_data[\"total_count\"]))\n",
    "print(\"Latest processing time: \"+str(local_data[\"latest_processing_time\"]))\n",
    "print(\"Example of a line from latest batch: \"+local_data[\"one_line\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.stop(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "batchIntervalSeconds = 10\n",
    "ssc = StreamingContext(sc, batchIntervalSeconds)\n",
    "lines=ssc.textFileStream(\"gs://big-data-streaming-examples\")\n",
    "ratings=lines.map(lambda x: json.loads(x)[\"rating\"]).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "rating_dist={}\n",
    "for i in range(11):\n",
    "    rating_dist[i]=0\n",
    "    \n",
    "def process(time, rdd):\n",
    "    for r in rdd.collect():\n",
    "        rating_dist[r[0]]+=r[1]\n",
    "    \n",
    "ratings.foreachRDD(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rating_dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c41bc74fa489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrating_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rating_dist' is not defined"
     ]
    }
   ],
   "source": [
    "rating_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r = range(11)\n",
    "\n",
    "frequency = list(map(lambda x: rating_dist[x], r))\n",
    " \n",
    "plt.bar(r, frequency, align='center', alpha=0.5)\n",
    "plt.xticks(r, r)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Rating frequency')\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}